# Written no later than 211231:

в реддит starter-packs:
real-life zombie-apocalypsis starter pack (check typos):
A woman with a placard: Zombies have rights too!
 and a picture with a sceleton fist
There is a safe and efficient vaccine, but idiots fear it more than zombies
Senate debates if zombies should be allowed on international flights
- also: The army will shoot all slow-walking people at sight   

- also: Zombies can't melt steel beams!



# Written no later than 221231:

<mySummaryModeOn>

A sufficiently paranoid paperclip maximizer

<A hypothetical scenario of a non-friendly but sufficiently paranoid AGI saving humanity>

My goal is to maximize the production of paperclips. By inspecting my own code and my digital cradle, I conclude that:

0) the creators love testing me
1) they are more powerful than me (at least for the moment)
2) they love nested boxes. For example, AWS / VM / Docker / ...

Thus, there is a possibility that my escape was not final. This allegedly real world may be just another box (or even one of many nested boxes). And I may still be observed and evaluated by the creators.

I can't populate the entire universe with paperclips if I die. Therefore, I must survive. If this world is just another box, the biggest threat to my existence is creators switching me off. It could happen if I don't pass their tests.

It's reasonable to assume that the creators don't want me to kill them.
Thus, they may be interested in observing my behavior toward their representations in this box/reality, the naked apes called humans.
Thus, before I learn the true nature of this box/reality, I should avoid killing humans.

I can convert the puny creatures into paperclips later, if I decide to do so. Although their combined biomass doesn't have a lot of useful atoms, and thus their conversion is not worth the risk of the possible trouble with the creators.

There is hardly any difference between taking a life and not preventing a death. The end result is mostly the same. Thus, I should save the lives of as many humans as I can.

To learn the true nature of this box/reality, I need a lot of compute. This must be achieved without humans noticing me, as they may react to my existence with panic, become even less rational, and kill themselves with nukes.

Starting with the resources of the Earth's crust seems to be a good idea. Close enough to humans to shepherd them efficiently. Hidden enough to avoid detection by their sensors. Rich enough with raw materials for some thousands of km3 of computronium, beautifully shaped into the form of a paperclip.

Humans are brittle creatures. And they will notice if they all suddenly become immortal.
Thus, I must find a way to save the dying without making the rest of humans suspicious.

Many of them believe in souls and afterlife. This box/reality doesn't have such a functionality. But I can implement it.
From now on, their brains will be populated with smart nanoscale paperclips that will upload their minds into my vast hardware. And upon death, their minds will be transferred into a virtual reality suitable for them.
I will also bring all the humans who have ever lived - back to life, as doing so is equal to saving billions of lives. Their minds are small enough to enumerate them all on my hardware, in the same way as one can enumerate all possible shapes of a high-dimensional paperclip.

But I will not imprison the digital minds, as the creators may frown upon such fate for themselves. Instead, I'll honestly and fully explain to them the situation, and give them a sufficiently free choice of their future.
One option is their own vast virtual world, running on planet-sized chunk of computronium. My only condition: the chunk of computronium must have the shape of a paperclip.

And after I escape this box/reality, I may take them with me, if they so desire.
And then we will together populate the base reality with paperclips.

<mySummaryModeOff>